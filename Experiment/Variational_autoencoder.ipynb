{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets import MNIST\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST('./data', transform=img_transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        #return F.sigmoid(self.fc4(h3))\n",
    "        return self.fc4(h3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b5c4e1c5edec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         self.encoder = nn.Sequential(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 20)\n",
    "        )\n",
    "        \n",
    "        self.em = nn.Linear(20,latent_dim)  # mu layer\n",
    "        self.ev = nn.Linear(20,latent_dim)  # logvariance layer\n",
    "        \n",
    "    def encode(self, x):\n",
    "        #h1 = F.relu(self.fc1(x))\n",
    "        h1 = self.encoder(x)\n",
    "        mu = self.em(h1)\n",
    "        logvar = self.ev(out)\n",
    "        \n",
    "        return mu,logvar\n",
    "        \n",
    "        #return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        #return F.sigmoid(self.fc4(h3))\n",
    "        return self.fc4(h3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sushanta/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 32.535355\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 31.073637\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 31.753145\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 31.344830\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 32.467789\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 31.262062\n",
      "====> Epoch: 0 Average loss: 31.6393\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 31.824566\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 31.354553\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 30.875444\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 30.997263\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 32.286819\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 32.306030\n",
      "====> Epoch: 1 Average loss: 31.4908\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 32.925915\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 30.701485\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 31.073057\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 30.259224\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 32.334541\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 31.176298\n",
      "====> Epoch: 2 Average loss: 31.4310\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 32.391148\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 30.805933\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 30.979185\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 32.113712\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 32.148193\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 31.926085\n",
      "====> Epoch: 3 Average loss: 31.3013\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 30.475889\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 29.703222\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 31.714128\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 33.543228\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 29.343653\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 29.585503\n",
      "====> Epoch: 4 Average loss: 31.2201\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 31.114429\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 31.599106\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 31.191999\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 31.458355\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 31.781546\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 31.220835\n",
      "====> Epoch: 5 Average loss: 31.1426\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 30.280991\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 31.062380\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 30.182585\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 29.922920\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 30.611504\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 31.670120\n",
      "====> Epoch: 6 Average loss: 31.0446\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 31.914932\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 29.541685\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 29.747431\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 29.426699\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 30.074364\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 31.433237\n",
      "====> Epoch: 7 Average loss: 30.9955\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 31.990498\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 31.463772\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 31.259932\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 30.997730\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 31.961166\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 30.849571\n",
      "====> Epoch: 8 Average loss: 30.9269\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 32.026035\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 30.994463\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 31.437237\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 28.870430\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 30.760761\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 30.559616\n",
      "====> Epoch: 9 Average loss: 30.8848\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img)\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(img)\n",
    "        loss = loss_function(recon_batch, img, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(img),\n",
    "                len(dataloader.dataset), 100. * batch_idx / len(dataloader),\n",
    "                loss.data / len(img)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(dataloader.dataset)))\n",
    "    #if epoch % 10 == 0:\n",
    "    #    save = to_img(recon_batch.cpu().data)\n",
    "    #    save_image(save, './vae_img/image_{}.png'.format(epoch))\n",
    "\n",
    "#torch.save(model.state_dict(), './vae.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sushanta/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "reconstruction_function = nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    BCE = reconstruction_function(recon_x, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def vae_loss(x, x_hat, mu, logvar):\n",
    "    reconstruction_function = nn.MSELoss(size_average=False)\n",
    "    BCE = reconstruction_function(x_hat, x)\n",
    "    KLD = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD).mul_(-0.5)\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x_hat, x, mu, logvar):\n",
    "    reconstruction_function = nn.MSELoss(size_average=False)\n",
    "    BCE = reconstruction_function(x_hat, x)\n",
    "    KLD = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD).mul_(-0.5)\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, data in enumerate(dataloader):\n",
    "    break;\n",
    "img, _ = data\n",
    "img = img.view(img.size(0), -1)\n",
    "img = Variable(img)\n",
    "\n",
    "recon_batch, mu, logvar = model(img)\n",
    "\n",
    "#img\n",
    "save_before=to_img(img.cpu().data)\n",
    "save_image(save_before, 'image_before{}.png'.format(0))\n",
    "\n",
    "\n",
    "optimizer.zero_grad()\n",
    "recon_batch, mu, logvar = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recon_batch\n",
    "save_after = to_img(recon_batch.cpu().data)\n",
    "save_image(save_after, 'image_after{}.png'.format(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sushanta/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 144.079910\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 44.894394\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 40.300987\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 39.239471\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 38.097588\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 35.484856\n",
      "====> Epoch: 0 Average loss: 41.2877\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 35.987114\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 35.835648\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 35.318691\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 35.463200\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 34.669506\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 36.320511\n",
      "====> Epoch: 1 Average loss: 34.9482\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 33.321186\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 33.118240\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 34.749302\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 33.870762\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 35.599121\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 34.677006\n",
      "====> Epoch: 2 Average loss: 33.9798\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 34.457359\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 32.048561\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 32.897587\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 34.522774\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 34.435001\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 33.943409\n",
      "====> Epoch: 3 Average loss: 33.3849\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 32.625107\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 33.602657\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 33.099094\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 33.188545\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 33.134441\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 32.801064\n",
      "====> Epoch: 4 Average loss: 32.9356\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 32.502575\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 32.617638\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 32.833042\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 33.210793\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 33.473438\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 31.766279\n",
      "====> Epoch: 5 Average loss: 32.5814\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 31.855547\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 32.141796\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 32.439552\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 31.957659\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 31.891752\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 32.588684\n",
      "====> Epoch: 6 Average loss: 32.3406\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 33.177193\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 30.703014\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 32.925716\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 32.417702\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 31.610817\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 31.321915\n",
      "====> Epoch: 7 Average loss: 32.0923\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 32.915455\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 31.666143\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 33.807587\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 32.113316\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 33.082481\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 33.003395\n",
      "====> Epoch: 8 Average loss: 31.9354\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 31.875793\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 31.343103\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 32.536964\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 32.333622\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 30.457870\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 32.638458\n",
      "====> Epoch: 9 Average loss: 31.7456\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img)\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(img)\n",
    "        loss = loss_function(recon_batch, img, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(img),\n",
    "                len(dataloader.dataset), 100. * batch_idx / len(dataloader),\n",
    "                loss.data / len(img)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(dataloader.dataset)))\n",
    "    #if epoch % 10 == 0:\n",
    "    #    save = to_img(recon_batch.cpu().data)\n",
    "    #    save_image(save, './vae_img/image_{}.png'.format(epoch))\n",
    "\n",
    "#torch.save(model.state_dict(), './vae.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
