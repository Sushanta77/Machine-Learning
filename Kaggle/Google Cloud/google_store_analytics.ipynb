{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.io.json import json_normalize\n",
    "import os\n",
    "import json\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col = ['transactionRevenue_size','hits_sum','hits_min','hits_max','hits_mean','hits_median','pageviews_sum','pageviews_min','pageviews_max','pageviews_mean','pageviews_median','bounces_sum','bounces_mean','bounces_median','newVisits_sum','newVisits_mean','newVisits_median','channelGrouping_min','channelGrouping_max','channelGrouping_mean','channelGrouping_median','channelGrouping_std','channelGrouping_var','socialEngagementType_min','socialEngagementType_max','socialEngagementType_mean','socialEngagementType_median','socialEngagementType_std','socialEngagementType_var','browser_min','browser_max','browser_mean','browser_median','browser_std','browser_var','deviceCategory_min','deviceCategory_max','deviceCategory_mean','deviceCategory_median','deviceCategory_std','deviceCategory_var','operatingSystem_min','operatingSystem_max','operatingSystem_mean','operatingSystem_median','operatingSystem_std','operatingSystem_var','city_min','city_max','city_mean','city_median','city_std','city_var','continent_min','continent_max','continent_mean','continent_median','continent_std','continent_var','country_min','country_max','country_mean','country_median','country_std','country_var','metro_min','metro_max','metro_mean','metro_median','metro_std','metro_var','networkDomain_min','networkDomain_max','networkDomain_mean','networkDomain_median','networkDomain_std','networkDomain_var','region_min','region_max','region_mean','region_median','region_std','region_var','subContinent_min','subContinent_max','subContinent_mean','subContinent_median','subContinent_std','subContinent_var','medium_min','medium_max','medium_mean','medium_median','medium_std','medium_var','source_min','source_max','source_mean','source_median','source_std','source_var','sess_date_dow_min','sess_date_dow_max','sess_date_dow_mean','sess_date_dow_median','sess_date_dow_std','sess_date_dow_var','sess_date_doy_min','sess_date_doy_max','sess_date_doy_mean','sess_date_doy_median','sess_date_doy_std','sess_date_doy_var','sess_date_mon_min','sess_date_mon_max','sess_date_mon_mean','sess_date_mon_median','sess_date_mon_std','sess_date_mon_var','sess_date_week_min','sess_date_week_max','sess_date_week_mean','sess_date_week_median','sess_date_week_std','sess_date_week_var','date_diff']\n",
    "#test_users[['transactionRevenue_size','hits_sum','hits_min','hits_max','hits_mean','hits_median','pageviews_sum','pageviews_min','pageviews_max','pageviews_mean','pageviews_median','bounces_sum','bounces_mean','bounces_median','newVisits_sum','newVisits_mean','newVisits_median','channelGrouping_min','channelGrouping_max','channelGrouping_mean','channelGrouping_median','channelGrouping_std','channelGrouping_var','socialEngagementType_min','socialEngagementType_max','socialEngagementType_mean','socialEngagementType_median','socialEngagementType_std','socialEngagementType_var','browser_min','browser_max','browser_mean','browser_median','browser_std','browser_var','deviceCategory_min','deviceCategory_max','deviceCategory_mean','deviceCategory_median','deviceCategory_std','deviceCategory_var','operatingSystem_min','operatingSystem_max','operatingSystem_mean','operatingSystem_median','operatingSystem_std','operatingSystem_var','city_min','city_max','city_mean','city_median','city_std','city_var','continent_min','continent_max','continent_mean','continent_median','continent_std','continent_var','country_min','country_max','country_mean','country_median','country_std','country_var','metro_min','metro_max','metro_mean','metro_median','metro_std','metro_var','networkDomain_min','networkDomain_max','networkDomain_mean','networkDomain_median','networkDomain_std','networkDomain_var','region_min','region_max','region_mean','region_median','region_std','region_var','subContinent_min','subContinent_max','subContinent_mean','subContinent_median','subContinent_std','subContinent_var','medium_min','medium_max','medium_mean','medium_median','medium_std','medium_var','source_min','source_max','source_mean','source_median','source_std','source_var','sess_date_dow_min','sess_date_dow_max','sess_date_dow_mean','sess_date_dow_median','sess_date_dow_std','sess_date_dow_var','sess_date_doy_min','sess_date_doy_max','sess_date_doy_mean','sess_date_doy_median','sess_date_doy_std','sess_date_doy_var','sess_date_mon_min','sess_date_mon_max','sess_date_mon_mean','sess_date_mon_median','sess_date_mon_std','sess_date_mon_var','sess_date_week_min','sess_date_week_max','sess_date_week_mean','sess_date_week_median','sess_date_week_std','sess_date_week_var','date_diff']].to_csv('test_users_mine.csv')\n",
    "#usecols=['channelGrouping', 'date', 'fullVisitorId', 'sessionId', 'visitId','visitNumber','visitStartTime','totals', 'device', 'geoNetwork', 'socialEngagementType', 'trafficSource'],\n",
    "\n",
    "def load_df(type='train',csv_path='input/train.csv', nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str',\n",
    "                           'date': 'str'\n",
    "                           }, # Important!!\n",
    "                     usecols=['channelGrouping', 'date', 'fullVisitorId', 'sessionId', 'visitId','visitNumber','visitStartTime','totals', 'device', 'geoNetwork', 'socialEngagementType', 'trafficSource'],\n",
    "                     nrows=nrows)\n",
    "    \n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    \n",
    "    df.fillna(0,inplace=True)\n",
    "    \n",
    "    if (type == 'test'):\n",
    "        df['totals.transactionRevenue'] = 0\n",
    "    \n",
    "    print(\"Loaded CSV For : {} - Shape : {}\".format(type,df.shape))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def date_feature(type,input_df='train'):\n",
    "    date_feat = ['sess_date_hours','sess_date_dom','sess_date_dow','sess_date_doy','sess_date_mon','sess_date_week']\n",
    "    \n",
    "    input_df['date']=pd.to_datetime(input_df['date'])\n",
    "    input_df[date_feat[0]] = input_df['date'].dt.hour\n",
    "    input_df[date_feat[1]] = input_df['date'].dt.day\n",
    "    input_df[date_feat[2]] = input_df['date'].dt.dayofweek\n",
    "    input_df[date_feat[3]] = input_df['date'].dt.dayofyear\n",
    "    input_df[date_feat[4]] = input_df['date'].dt.month\n",
    "    input_df[date_feat[5]] = input_df['date'].dt.weekofyear\n",
    "    \n",
    "    print (\"Date Feature Completed For: {} - Shape:{}\".format(type,input_df.shape))\n",
    "    return input_df,date_feat\n",
    "\n",
    "\n",
    "def factorize_categorical (type='train',input_df='train',cat_indexer=None):\n",
    "    cat_feat=['channelGrouping','socialEngagementType',\n",
    "              'device.browser','device.deviceCategory',\n",
    "              'device.operatingSystem','geoNetwork.city',\n",
    "              'geoNetwork.continent','geoNetwork.country',\n",
    "              'geoNetwork.metro','geoNetwork.networkDomain',\n",
    "              'geoNetwork.region','geoNetwork.subContinent',\n",
    "              'trafficSource.adContent','trafficSource.campaign', \n",
    "              'trafficSource.isTrueDirect','trafficSource.keyword', \n",
    "              'trafficSource.medium','trafficSource.referralPath', \n",
    "              'trafficSource.source']\n",
    "\n",
    "    # 'trafficSource.adContent','trafficSource.adwordsClickInfo.adNetworkType','trafficSource.adwordsClickInfo.gclId'\n",
    "    # 'trafficSource.adwordsClickInfo.isVideoAd','trafficSource.adwordsClickInfo.page','trafficSource.adwordsClickInfo.slot'\n",
    "    # 'trafficSource.campaign','trafficSource.isTrueDirect','trafficSource.keyword','trafficSource.referralPath'\n",
    "    \n",
    "    num_feat = ['totals.bounces','totals.hits','totals.newVisits','totals.pageviews','totals.transactionRevenue']\n",
    "    \n",
    "    if (cat_indexer is None):\n",
    "        cat_indexer = {}\n",
    "        for col in cat_feat:\n",
    "            #print (\"Factorizing Column: {}\".format(col))\n",
    "            input_df[col],indexer = pd.factorize(input_df[col])\n",
    "            cat_indexer[col]=indexer\n",
    "    else:\n",
    "        for col in cat_feat:\n",
    "            #print (\"Factorizing Column: {}\".format(col))\n",
    "            input_df[col] = cat_indexer[col].get_indexer(input_df[col])\n",
    "        \n",
    "    for col in num_feat:\n",
    "        #if ( (type == 'train') or (type == 'test' and col != 'totals.transactionRevenue') ):\n",
    "        #print (\"Converting Column to float:{}\".format(col))\n",
    "        input_df[col]=input_df[col].astype(float)\n",
    "            \n",
    "    #if (type == 'train'):\n",
    "    input_df['totals.transactionRevenue'].fillna(0,inplace=True)\n",
    "    \n",
    "    print (\"Factorize Completed For: {} - Shape:{}\".format(type,input_df.shape))\n",
    "    \n",
    "    return input_df,cat_feat,num_feat,cat_indexer\n",
    "\n",
    "\n",
    "def summrize_by_user(type='train',input_df='train',cat_feat=None,num_feat=None,date_feat=None):\n",
    "    aggs = {}\n",
    "    y = []\n",
    "    \n",
    "    aggs = {\n",
    "        'date': ['min', 'max'],\n",
    "        'totals.transactionRevenue': ['sum', 'size'],\n",
    "        'totals.hits': ['sum', 'min', 'max', 'mean', 'median'],\n",
    "        'totals.pageviews': ['sum', 'min', 'max', 'mean', 'median'],\n",
    "        'totals.bounces': ['sum', 'mean', 'median'],\n",
    "        'totals.newVisits': ['sum', 'mean', 'median']\n",
    "    }\n",
    "        \n",
    "    #if (type == 'train'):\n",
    "        #aggs['totals.transactionRevenue'] = ['sum','size']\n",
    "    \n",
    "    \"\"\"\"\n",
    "    for num_col in num_feat:\n",
    "        if (num_col != 'totals.transactionRevenue'):\n",
    "            aggs[num_col] = ['sum','min','max','mean','median']\n",
    "    \"\"\"\n",
    "    \n",
    "    for cat_col in cat_feat:\n",
    "        aggs[cat_col] = ['min','max','mean','median','std','var']\n",
    "        \n",
    "    for date_col in date_feat:\n",
    "        aggs[date_col] = ['min','max','mean','median','std','var']\n",
    "        \n",
    "    users=input_df.groupby('fullVisitorId').agg(aggs)\n",
    "    \n",
    "    new_col = [l1+'_'+l2 for l1 in aggs.keys() for l2 in aggs[l1]]\n",
    "    users.columns = new_col\n",
    "    \n",
    "    #if (type == 'train'):\n",
    "    users['totals.transactionRevenue_sum'] = np.log1p(users['totals.transactionRevenue_sum'])\n",
    "    y = users['totals.transactionRevenue_sum']\n",
    "        #users.drop(['totals.transactionRevenue_sum'],axis=1,inplace=True)\n",
    "    \n",
    "    users['date_diff'] = (users.date_max - users.date_min).astype(np.int64) // (24 * 3600 * 1e9)\n",
    "    \n",
    "    users.drop(['date_min','date_max','totals.transactionRevenue_sum'], axis=1, inplace=True)\n",
    "\n",
    "    print (\"Aggregation Completed For:{} - Shape:{}\".format(type,users.shape))\n",
    "    \n",
    "    return users,y\n",
    "\n",
    "\n",
    "\n",
    "def get_KFold(n_splits=5,shuffle=True,random_state=7):\n",
    "    KFolds = KFold(n_splits=n_splits,shuffle=True,random_state=7)\n",
    "    \n",
    "    #folds=KFolds.split(df)\n",
    "    \n",
    "    return KFolds\n",
    "\n",
    "\n",
    "\n",
    "def get_GroupKFolds(df=None, n_splits=5):\n",
    "    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n",
    "    # Get sorted unique visitors\n",
    "    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n",
    "\n",
    "    # Get folds\n",
    "    folds = GroupKFold(n_splits=n_splits)\n",
    "    fold_ids = []\n",
    "    ids = np.arange(df.shape[0])\n",
    "    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n",
    "        fold_ids.append(\n",
    "            [\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return fold_ids\n",
    "\n",
    "\n",
    "\n",
    "def rf_kfold(train_X,train_y,test_X,n_estimators=20000,max_depth=12,verbose=2,n_splits=10):\n",
    "    print (\"Train & Predict using Random Forest with Splits:{}\".format(n_splits))\n",
    "    val_pred = np.zeros((train_X.shape[0]))\n",
    "    test_pred = np.zeros((test_X.shape[0]))\n",
    "    \n",
    "    rf_feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    Folds = KFold(n_splits=n_splits,shuffle=True,random_state=7)\n",
    "\n",
    "    params = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'criterion' : 'mse',\n",
    "        'max_depth' : max_depth ,\n",
    "        'random_state': 123,\n",
    "        'verbose': verbose\n",
    "    }\n",
    "\n",
    "    for fold_,(train_,val_) in enumerate(Folds.split(train_X)):\n",
    "        rf = RandomForestRegressor()\n",
    "        rf.set_params(**params)\n",
    "        \n",
    "        X = train_X.iloc[train_]\n",
    "        y = train_y.iloc[train_]\n",
    "        rf.fit(X,y)\n",
    "        \n",
    "        val_pred[val_]=rf.predict(train_X.iloc[val_])\n",
    "        \n",
    "        curr_test_pred=rf.predict(test_X)\n",
    "        curr_test_pred[curr_test_pred < 0] = 0\n",
    "        test_pred += curr_test_pred / n_splits\n",
    "        print (\"Fold: {}   RMSE:{}\".format(fold_,mean_squared_error(val_pred[val_],train_y[val_])**0.5) )\n",
    "        \n",
    "        fold_feature_importance_df = pd.DataFrame()\n",
    "        fold_feature_importance_df[\"feature\"] = train_X.columns\n",
    "        fold_feature_importance_df[\"importance\"] = rf.feature_importances_\n",
    "        fold_feature_importance_df[\"fold\"] = fold_ + 1\n",
    "        rf_feature_importance_df = pd.concat([rf_feature_importance_df,fold_feature_importance_df],axis=0)\n",
    "        \n",
    "    val_pred[val_pred<0] = 0\n",
    "    \n",
    "    print (\"Fold: A   RMSE:{} \".format(mean_squared_error(val_pred,train_y)**0.5) )\n",
    "    \n",
    "    return val_pred,test_pred,rf_feature_importance_df\n",
    "\n",
    "\n",
    "\n",
    "def lgb_kfold(train_X=None,train_y=None,test_X=None,features=None,learning_rate=0.01,n_estimators=20000,num_leaves=128,\n",
    "              subsample=0.2217,colsample_bytree=0.6810,min_split_gain=np.power(10.0, -4.9380),\n",
    "              reg_alpha=np.power(10.0, -3.2454),reg_lambda=np.power(10.0, -4.8571),min_child_weight=np.power(10.0, 2),\n",
    "              early_stopping_rounds=100,verbose=False,foldType='Group',n_splits=10):\n",
    "    \n",
    "    print (\"Train & Predict using Light GBM with Splits:{}\".format(n_splits))\n",
    "    \n",
    "    val_pred = np.zeros((train_X.shape[0]))\n",
    "    test_pred = np.zeros((test_X.shape[0]))\n",
    "    \n",
    "    lgb_feature_importance_df = pd.DataFrame()\n",
    "    lgb_validation_loss_df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    if (foldType == 'Group'):\n",
    "        Folds = get_GroupKFolds(df=train_X, n_splits=n_splits)\n",
    "        folds = Folds\n",
    "        \n",
    "    else:\n",
    "        Folds = get_KFold(n_splits=n_splits,shuffle=True,random_state=7)\n",
    "        folds = Folds.split(train_X)\n",
    "    \n",
    "    \n",
    "    params = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators': n_estimators,\n",
    "        'num_leaves': num_leaves,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'min_split_gain': min_split_gain,\n",
    "        'reg_alpha': reg_alpha,\n",
    "        'reg_lambda': reg_lambda,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'silent': True\n",
    "    }\n",
    "        \n",
    "    for fold_,(train_,val_) in enumerate(folds):\n",
    "        lgb_model = lgb.LGBMRegressor(**params)\n",
    "        \n",
    "        lgb_model.fit(\n",
    "            X=train_X[features].iloc[train_],\n",
    "            y=train_y.iloc[train_],\n",
    "            eval_set=[(train_X[features].iloc[train_],train_y.iloc[train_]),(train_X[features].iloc[val_],train_y.iloc[val_])],\n",
    "            eval_metric='rmse',\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose=verbose\n",
    "        )\n",
    "            \n",
    "        val_pred[val_]=lgb_model.predict(train_X[features].iloc[val_])\n",
    "        curr_test_pred=lgb_model.predict(test_X[features])\n",
    "        curr_test_pred[curr_test_pred < 0] = 0\n",
    "        test_pred += curr_test_pred / n_splits\n",
    "        print (\"Fold: {}   RMSE:{}   Best Iteration:{}\".format(fold_,mean_squared_error(val_pred[val_],train_y[val_])**0.5,lgb_model.best_iteration_) )\n",
    "        \n",
    "        fold_feature_importance_df = pd.DataFrame()\n",
    "        fold_feature_importance_df[\"feature\"] = train_X[features].columns\n",
    "        fold_feature_importance_df[\"importance\"] = lgb_model.feature_importances_\n",
    "        fold_feature_importance_df[\"fold\"] = fold_ + 1\n",
    "        lgb_feature_importance_df = pd.concat([lgb_feature_importance_df,fold_feature_importance_df],axis=0)\n",
    "        \n",
    "        lgb_fold_validation_loss_df = pd.DataFrame()\n",
    "        lgb_fold_validation_loss_df['rmse']= lgb_model.evals_result_['valid_1']['rmse']\n",
    "        lgb_fold_validation_loss_df['fold'] = fold_+1\n",
    "        lgb_validation_loss_df = pd.concat([lgb_fold_validation_loss_df,lgb_validation_loss_df],axis=0)\n",
    "        \n",
    "        \n",
    "    val_pred[val_pred<0] = 0\n",
    "    \n",
    "    print (\"Fold: A   RMSE:{}   Best Iteration:None\".format(mean_squared_error(val_pred,train_y)**0.5) )\n",
    "    \n",
    "    return val_pred,test_pred,lgb_feature_importance_df,lgb_validation_loss_df\n",
    "\n",
    "\n",
    "    \n",
    "def xgb_kfold(train_X,train_y,test_X,n_splits=10):\n",
    "    print (\"Train & Predict using XGBOOST with Splits:{}\".format(n_splits))\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    Folds = KFold(n_splits=n_splits,shuffle=True,random_state=7)\n",
    "    \n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        max_depth=12,\n",
    "        learning_rate=0.003,\n",
    "        booster='gbtree',\n",
    "        objective='reg:linear',\n",
    "        reg_lambda=0.8,\n",
    "        min_child_weight=4,\n",
    "        subsample=0.3,\n",
    "        colsample_bytree=0.5,\n",
    "        n_estimators=20000,\n",
    "        silent=True,\n",
    "        seed=239)\n",
    "    \n",
    "    val_pred = np.zeros((train_X.shape[0]))\n",
    "    test_pred = np.zeros((test_X.shape[0]))\n",
    "\n",
    "    for fold_,(train_,val_) in enumerate(Folds.split(train_X)):\n",
    "        xgb_model.fit(\n",
    "            X = train_X.iloc[train_],\n",
    "            y = train_y[train_],\n",
    "            eval_metric='rmse',\n",
    "            eval_set=[(train_X.iloc[train_],train_y[train_]),(train_X.iloc[val_],train_y[val_])],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=200\n",
    "        )\n",
    "\n",
    "        val_pred[val_]=xgb_model.predict(train_X.iloc[val_])\n",
    "        curr_test_pred=xgb_model.predict(test_X)\n",
    "        curr_test_pred[curr_test_pred < 0] = 0\n",
    "        test_pred += curr_test_pred / n_splits\n",
    "        print (\"Fold: {}   RMSE:{}   Best Iteration:{}\".format(fold_,mean_squared_error(val_pred[val_],train_y[val_])**0.5,xgb_model.best_iteration) )\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = train_X.columns\n",
    "        fold_importance_df[\"importance\"] = xgb_model.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df,fold_importance_df],axis=0)\n",
    "\n",
    "        \n",
    "    val_pred[val_pred<0] = 0\n",
    "    \n",
    "    print (\"Fold: A   RMSE:{}   Best Iteration:None\".format(mean_squared_error(val_pred,train_y)**0.5) )\n",
    "\n",
    "    return val_pred,test_pred,feature_importance_df\n",
    "\n",
    "\n",
    "\n",
    "def rmse_min_func(weights):\n",
    "    ''' scipy minimize will pass the weights as a numpy array '''\n",
    "    final_prediction = 0\n",
    "    for weight, prediction in zip(weights, blend_train):\n",
    "        final_prediction += weight*prediction\n",
    "        \n",
    "    return np.sqrt(mean_squared_error(trn_y,final_prediction))\n",
    "\n",
    "\n",
    "\n",
    "def display_loss(model_validation_loss_df=None,title='Boosting Iteration Vs RMSE'):\n",
    "    print (\"Displaying Metric Loss\")\n",
    "    plt.figure(figsize=(18,12))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Boosting Iteration')\n",
    "    plt.ylabel('RMSE')\n",
    "    for k in np.arange(len(model_validation_loss_df['fold'].value_counts())):\n",
    "        plt.plot(np.arange(len(model_validation_loss_df[model_validation_loss_df['fold'] == k+1]['rmse'])),model_validation_loss_df[model_validation_loss_df['fold'] == k+1]['rmse'],label='Fold {}'.format(k))\n",
    "    \n",
    "    plt.legend()\n",
    "        \n",
    "\n",
    "\n",
    "def display_feature_importance(feature_importance_df_=None,modelName=\"LightGBM Feature Importance (Over Folds)\"):\n",
    "    print (\"Displaying Feature Importance\")\n",
    "    \n",
    "    cols = feature_importance_df_[['feature','importance']].groupby(['feature']).mean().sort_values(\n",
    "        by='importance',ascending=False)[:50].index\n",
    "    \n",
    "    best_features = feature_importance_df_[feature_importance_df_['feature'].isin(cols)]\n",
    "    \n",
    "    plt.figure(figsize=(12,20))\n",
    "    sns.barplot(x=\"importance\",y=\"feature\",data=best_features.sort_values(by='importance',ascending=False))\n",
    "    plt.title(modelName)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/lgbm_feature_importance.png')\n",
    "    \n",
    "    \n",
    "    \n",
    "def write_submission_file(test_X=None,test_pred=None,filename=None):\n",
    "    print (\"Write the Submission File\")\n",
    "    \n",
    "    test_X['PredictedLogRevenue'] = test_pred\n",
    "    print (\"Start Creating Submission File\")\n",
    "    \n",
    "    test_X[['PredictedLogRevenue']].to_csv(\"submission/{}_{}.csv\".format(filename,datetime.datetime.now().strftime('%Y%m%d_%H%M%S')))\n",
    "    \n",
    "    test_X.drop('PredictedLogRevenue',axis=1,inplace=True)\n",
    "\n",
    "    return 'Success'\n",
    "\n",
    "\n",
    "def blending_weights(blend_train=None):\n",
    "    blend_train = []\n",
    "    blend_test = []\n",
    "    \n",
    "    xgboost_val_pred.columns = ['val_pred']\n",
    "    lgboost_val_pred.columns = ['val_pred']\n",
    "    xgboost_test_pred.columns = ['val_pred']\n",
    "    lgboost_test_pred.columns = ['val_pred']\n",
    "    \n",
    "    blend_train.append(xgboost_val_pred['val_pred'].values)\n",
    "    blend_train.append(lgboost_val_pred['val_pred'].values)\n",
    "    \n",
    "    blend_test.append(xgboost_test_pred['val_pred'].values)\n",
    "    blend_test.append(lgboost_test_pred['val_pred'].values)\n",
    "    \n",
    "    \n",
    "    #Run the minimize function for a certain number\n",
    "    print('\\nFinding Blending Weights ...')\n",
    "    res_list = []\n",
    "    weights_list = []\n",
    "    num_rounds_ensemble = 5000\n",
    "    \n",
    "    for k in range (num_rounds_ensemble):\n",
    "        \n",
    "        starting_values = np.random.uniform(size=len(blend_train))\n",
    "        \n",
    "    \n",
    "        #######\n",
    "        # https://www.kaggle.com/tilii7/cross-validation-weighted-linear-blending-errors\n",
    "        # I used to think that weights should not be negative - many agree with that.\n",
    "        # I've come around on that issues as negative weights sometimes do help.\n",
    "        # If you don't think so, just swap the two lines below.\n",
    "        #######\n",
    "\n",
    "        #    bounds = [(0, 1)]*len(blend_train)\n",
    "        bounds = [(-1, 1)] * len(blend_train)\n",
    "\n",
    "        res = minimize(rmse_min_func,\n",
    "                       starting_values,\n",
    "                       method='L-BFGS-B',\n",
    "                       bounds=bounds,\n",
    "                       options={'disp': False,\n",
    "                                'maxiter': 100000})\n",
    "        res_list.append(res['fun'])\n",
    "        weights_list.append(res['x'])\n",
    "        print (\"Iteration:{}\\tScore:{}\\tWeights:{}\".format(k+1,res['fun'],([str(item) for item in res['x']])))\n",
    "\n",
    "        bestSC = np.min(res_list)\n",
    "        bestWght = weights_list[np.argmin(res_list)]\n",
    "        weights = bestWght\n",
    "        blend_score = round(bestSC, 6)\n",
    "\n",
    "        print('\\nEnsemble Score: {best_score}'.format(best_score=bestSC))\n",
    "        print('\\nBest Weights: {weights}'.format(weights=bestWght))\n",
    "\n",
    "        test_ensemble = np.zeros(len(blend_test[0]))\n",
    "        for k in range(len(blend_test)):\n",
    "            print(\"Model: {} * Weight:{}\".format(k+1, weights[k]))\n",
    "            test_ensemble += blend_test[k] * weights[k]\n",
    "    \n",
    "    \n",
    "def bayes_parameter_opt_lgb(X,y,num_boost_round=20000,nfold=5,early_stopping_rounds=100,metrics='rmse',random_state=1319):\n",
    "    train_data = lgb.Dataset(data=X, label=y)\n",
    "    \n",
    "    '''Function for Light GBM Evaluate Function'''\n",
    "    def lgb_eval(num_leaves,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 colsample_bytree,\n",
    "                 min_split_gain,\n",
    "                 reg_alpha,\n",
    "                 reg_lambda,\n",
    "                 min_child_weight\n",
    "                 ):\n",
    "    \n",
    "        params = {}\n",
    "\n",
    "        params[\"num_leaves\"] = round(num_leaves)\n",
    "        params['max_depth'] = round(max_depth)\n",
    "        params['subsample'] = subsample\n",
    "        params['colsample_bytree'] = colsample_bytree\n",
    "        params['min_split_gain']=min_split_gain\n",
    "        params['reg_alpha'] = reg_alpha\n",
    "        params['reg_lambda'] = reg_lambda\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "\n",
    "\n",
    "        cv_result = lgb.cv(params,\n",
    "                           train_set=train_data,\n",
    "                           num_boost_round=num_boost_round,\n",
    "                           nfold=nfold,\n",
    "                           metrics=metrics,\n",
    "                           early_stopping_rounds=early_stopping_rounds,\n",
    "                           verbose_eval=1,\n",
    "                           seed=random_state)\n",
    "\n",
    "        return max(cv_result['auc-mean'])\n",
    "    '''End of Light GBM Evaluate Function'''\n",
    "    \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (24, 45),\n",
    "                                            'max_depth':(8,12),\n",
    "                                            'subsample':(0.1,0.4),\n",
    "                                            'colsample_bytree': (0.1,0.8),\n",
    "                                            'min_split_gain': (0.001, 0.1),                                            \n",
    "                                            'feature_fraction': (0.1, 0.9),\n",
    "                                            'reg_alpha': (0.0001, 0.0008),\n",
    "                                            'reg_lambda': (0.00001, 0.00008),\n",
    "                                            'min_child_weight': (5, 200)}, random_state=0)\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    return lgbBO.res['max']['max_params']    \n",
    "\n",
    "\n",
    "def lgb_evaluate(num_leaves,max_depth,subsample,colsample_bytree,min_split_gain,reg_alpha,reg_lambda,min_child_weight):\n",
    "\n",
    "    params = {}\n",
    "\n",
    "    params[\"num_leaves\"] = int(round(num_leaves))\n",
    "    #params[\"feature_fraction\"] = round(feature_fraction)\n",
    "    params['max_depth'] = int(round(max_depth))\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample_bytree\n",
    "    params['min_split_gain']=min_split_gain\n",
    "    params['reg_alpha'] = reg_alpha\n",
    "    params['reg_lambda'] = reg_lambda\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    \n",
    "    cv_result = lgb.cv(params,\n",
    "                       train_set=train_data,\n",
    "                       num_boost_round=num_boost_round,\n",
    "                       nfold=nfold,\n",
    "                       stratified=False,\n",
    "                       metrics=metrics,\n",
    "                       early_stopping_rounds=early_stopping_rounds,\n",
    "                       verbose_eval=200,\n",
    "                       seed=random_state\n",
    "                       )\n",
    "\n",
    "    return max(cv_result['rmse-mean'])\n",
    "\n",
    "\n",
    "    lgbBO = BayesianOptimization(lgb_evaluate, {'num_leaves': (24, 45),\n",
    "                                            'max_depth':(8,12),\n",
    "                                            'subsample':(0.1,0.4),\n",
    "                                            'colsample_bytree': (0.1,0.8),\n",
    "                                            'min_split_gain': (0.001, 0.1),                                            \n",
    "                                            'reg_alpha': (0.0001, 0.0008),\n",
    "                                            'reg_lambda': (0.00001, 0.00008),\n",
    "                                            'min_child_weight': (5, 200)}, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef main():\\n    cat_Feature=[]\\n    num_feature=[]\\n    \\n    train = load_df('Train','input/train.csv')\\n    test  = load_df('Test','input/test.csv')\\n\\n    train,date_feat = date_feature('Train',train)\\n    test,_  = date_feature('Test',test)    \\n    \\n    train,cat_feat,num_feat = factorize_categorical('Train',train)\\n    test,_,_  = factorize_categorical('Test',test)\\n    \\n    trn_users,trn_y = summrize_by_user('Train',train,cat_feat,num_feat,date_feat)\\n    test_users,_    = summrize_by_user('Test',test,cat_feat,num_feat,date_feat)\\n    \\n    val_pred,test_pred=lgb_kfold(train_X=trn_users,train_y=trn_y,test_X=test_users,n_splits=10)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def main():\n",
    "    cat_Feature=[]\n",
    "    num_feature=[]\n",
    "    \n",
    "    train = load_df('Train','input/train.csv')\n",
    "    test  = load_df('Test','input/test.csv')\n",
    "\n",
    "    train,date_feat = date_feature('Train',train)\n",
    "    test,_  = date_feature('Test',test)    \n",
    "    \n",
    "    train,cat_feat,num_feat = factorize_categorical('Train',train)\n",
    "    test,_,_  = factorize_categorical('Test',test)\n",
    "    \n",
    "    trn_users,trn_y = summrize_by_user('Train',train,cat_feat,num_feat,date_feat)\n",
    "    test_users,_    = summrize_by_user('Test',test,cat_feat,num_feat,date_feat)\n",
    "    \n",
    "    val_pred,test_pred=lgb_kfold(train_X=trn_users,train_y=trn_y,test_X=test_users,n_splits=10)\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV For : train - Shape : (903653, 55)\n",
      "Loaded CSV For : test - Shape : (804684, 54)\n",
      "Date Feature Completed For: train - Shape:(903653, 61)\n",
      "Date Feature Completed For: test - Shape:(804684, 60)\n",
      "Factorize Completed For: train - Shape:(903653, 61)\n",
      "Factorize Completed For: test - Shape:(804684, 60)\n",
      "CPU times: user 6min 29s, sys: 14.5 s, total: 6min 43s\n",
      "Wall time: 5min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cat_Feature=[]\n",
    "num_feature=[]\n",
    "\n",
    "train = load_df('train','../input/train.csv')\n",
    "test  = load_df('test','../input/test.csv')\n",
    "\n",
    "train,date_feat = date_feature('train',train)\n",
    "test,_  = date_feature('test',test)    \n",
    "\n",
    "train,cat_feat,num_feat,indexer = factorize_categorical('train',train,cat_indexer=None)\n",
    "test,_,_,_  = factorize_categorical('test',test,cat_indexer=indexer)\n",
    "\n",
    "#trn_users,trn_y = summrize_by_user('train',train,cat_feat,num_feat,date_feat)\n",
    "#test_users,_    = summrize_by_user('test',test,cat_feat,num_feat,date_feat)\n",
    "\n",
    "#val_pred,test_pred,feature_importance =lgb_kfold(train_X=trn_users,train_y=trn_y,test_X=test_users,n_splits=10)\n",
    "#write_submission_file(test_X=test_users,test_pred=test_pred,filename='lgb_kfold')\n",
    "\n",
    "#val_pred,test_pred,feature_importance =xgb_kfold(train_X=trn_users,train_y=trn_y,test_X=test_users,n_splits=10)\n",
    "#write_submission_file(test_X=test_users,test_pred=test_pred,filename='xgb_kfold')\n",
    "\n",
    "#display_feature_importance(feature_importance_df,modelName=\"Lightgbm Feature Importance (Over Folds)\")\n",
    "#display_feature_importance(feature_importance_df,modelName=\"XGBOOST Feature Importance (Over Folds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train & Predict using Light GBM with Splits:10\n",
      "Fold: 0   RMSE:1.6236771650239867   Best Iteration:639\n",
      "Fold: 1   RMSE:1.5746025237166217   Best Iteration:499\n",
      "Fold: 2   RMSE:1.5532708935327189   Best Iteration:661\n",
      "Fold: 3   RMSE:1.582063240404973   Best Iteration:556\n",
      "Fold: 4   RMSE:1.5871449289942965   Best Iteration:568\n",
      "Fold: 5   RMSE:1.5676297658817453   Best Iteration:715\n",
      "Fold: 6   RMSE:1.573098070782427   Best Iteration:598\n",
      "Fold: 7   RMSE:1.5508281741586716   Best Iteration:638\n",
      "Fold: 8   RMSE:1.5607572624316943   Best Iteration:673\n",
      "Fold: 9   RMSE:1.5933419967428744   Best Iteration:620\n",
      "Fold: A   RMSE:1.5765109076766548   Best Iteration:None\n"
     ]
    }
   ],
   "source": [
    "val_pred,test_pred,rf_feature_importance_df = rf_kfold(train_X=trn_users,train_y=trn_y,test_X=test_users,n_estimators=100,max_depth=6,verbose=2,n_splits=10)\n",
    "val_pred,test_pred,feature_importance,lgb_validation_loss_df=lgb_kfold(train_X=trn_users,train_y=trn_y,test_X=test_users,learning_rate=0.01,n_estimators=20000,early_stopping_rounds=100,verbose=False,n_splits=10)\n",
    "\n",
    "display_feature_importance(feature_importance_df_=feature_importance,modelName=\"LightGBM Feature Importance (Over Folds)\")\n",
    "display_loss(lgb_validation_loss_df,title='Lgbm Boosting Iteration Vs RMSE')\n",
    "\n",
    "write_submission_file(test_X=test_users,test_pred=test_pred,filename='xgb_kfold')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gc.enable\n",
    "    main()\n",
    "    print (\"Model Completed..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------\n",
    "# Experiment from Here\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=train['totals.transactionRevenue'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = ['channelGrouping', 'visitNumber',\n",
    "       'device.browser','device.deviceCategory','device.isMobile',\n",
    "       'device.operatingSystem','geoNetwork.city','geoNetwork.continent',\n",
    "       'geoNetwork.country','geoNetwork.metro','geoNetwork.networkDomain',\n",
    "       'geoNetwork.region', 'geoNetwork.subContinent', 'totals.bounces',\n",
    "       'totals.hits', 'totals.newVisits', 'totals.pageviews',\n",
    "       'trafficSource.adContent','trafficSource.campaign',\n",
    "       'trafficSource.isTrueDirect','trafficSource.keyword',\n",
    "       'trafficSource.medium','trafficSource.referralPath',\n",
    "       'trafficSource.source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train & Predict using Light GBM with Splits:10\n",
      "Fold: 0   RMSE:1.6284244444344143   Best Iteration:563\n",
      "Fold: 1   RMSE:1.6364796304531262   Best Iteration:551\n",
      "Fold: 2   RMSE:1.625245765299165   Best Iteration:859\n",
      "Fold: 3   RMSE:1.5856865953322508   Best Iteration:707\n",
      "Fold: 4   RMSE:1.6852066514882296   Best Iteration:697\n",
      "Fold: 5   RMSE:1.647896392597565   Best Iteration:698\n",
      "Fold: 6   RMSE:1.6091558837339293   Best Iteration:767\n",
      "Fold: 7   RMSE:1.63032083821995   Best Iteration:648\n",
      "Fold: 8   RMSE:1.6431750661506315   Best Iteration:748\n",
      "Fold: 9   RMSE:1.6206747349229396   Best Iteration:735\n",
      "Fold: A   RMSE:7.873284945260392e+40   Best Iteration:None\n",
      "CPU times: user 38min 16s, sys: 15min 39s, total: 53min 56s\n",
      "Wall time: 12min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_pred,test_pred,lgb_feature_importance_df,lgb_validation_loss_df = lgb_kfold(train_X=train,train_y=np.log1p(train_y),test_X=test,features=train_features,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train\n",
    "train_y = np.log1p(train_y)\n",
    "test_X = test\n",
    "features=train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = np.zeros((train_X.shape[0]))\n",
    "test_pred = np.zeros((test_X.shape[0]))\n",
    "\n",
    "lgb_feature_importance_df = pd.DataFrame()\n",
    "lgb_validation_loss_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Folds = get_GroupKFolds(df=train_X, n_splits=10                      )\n",
    "folds = Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 20000,\n",
    "    'num_leaves': 128,\n",
    "    'subsample': 0.2217,\n",
    "    'colsample_bytree': 0.6810,\n",
    "    'min_split_gain': np.power(10.0, -4.9380),\n",
    "    'reg_alpha': np.power(10.0, -3.2454),\n",
    "    'reg_lambda': np.power(10.0, -4.8571),\n",
    "    'min_child_weight': np.power(10.0, 2),\n",
    "    'silent': True\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0   RMSE:1.6284244444344143   Best Iteration:563\n",
      "Fold: 1   RMSE:1.6364796304531262   Best Iteration:551\n",
      "Fold: 2   RMSE:1.625245765299165   Best Iteration:859\n",
      "Fold: 3   RMSE:1.5856865953322508   Best Iteration:707\n",
      "Fold: 4   RMSE:1.6852066514882296   Best Iteration:697\n",
      "Fold: 5   RMSE:1.647896392597565   Best Iteration:698\n",
      "Fold: 6   RMSE:1.6091558837339293   Best Iteration:767\n",
      "Fold: 7   RMSE:1.63032083821995   Best Iteration:648\n",
      "Fold: 8   RMSE:1.6431750661506315   Best Iteration:748\n",
      "Fold: 9   RMSE:1.6206747349229396   Best Iteration:735\n",
      "Fold: A   RMSE:1.6311168247226826   Best Iteration:None\n",
      "Fold: A   RMSE:1.6311168247226826   Best Iteration:None\n"
     ]
    }
   ],
   "source": [
    "n_splits = 10\n",
    "for fold_,(train_,val_) in enumerate(folds):\n",
    "    lgb_model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "    lgb_model.fit(\n",
    "        X=train_X[features].iloc[train_],\n",
    "        y=train_y.iloc[train_],\n",
    "        eval_set=[(train_X[features].iloc[train_],train_y.iloc[train_]),(train_X[features].iloc[val_],train_y.iloc[val_])],\n",
    "        eval_metric='rmse',\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    val_pred[val_]=lgb_model.predict(train_X[features].iloc[val_])\n",
    "    curr_test_pred=lgb_model.predict(test_X[features])\n",
    "    curr_test_pred[curr_test_pred < 0] = 0\n",
    "    test_pred += curr_test_pred / n_splits\n",
    "    print (\"Fold: {}   RMSE:{}   Best Iteration:{}\".format(fold_,mean_squared_error(val_pred[val_],train_y[val_])**0.5,lgb_model.best_iteration_) )\n",
    "\n",
    "    fold_feature_importance_df = pd.DataFrame()\n",
    "    fold_feature_importance_df[\"feature\"] = train_X[features].columns\n",
    "    fold_feature_importance_df[\"importance\"] = lgb_model.feature_importances_\n",
    "    fold_feature_importance_df[\"fold\"] = fold_ + 1\n",
    "    lgb_feature_importance_df = pd.concat([lgb_feature_importance_df,fold_feature_importance_df],axis=0)\n",
    "\n",
    "    lgb_fold_validation_loss_df = pd.DataFrame()\n",
    "    lgb_fold_validation_loss_df['rmse']= lgb_model.evals_result_['valid_1']['rmse']\n",
    "    lgb_fold_validation_loss_df['fold'] = fold_+1\n",
    "    lgb_validation_loss_df = pd.concat([lgb_fold_validation_loss_df,lgb_validation_loss_df],axis=0)\n",
    "\n",
    "\n",
    "val_pred[val_pred<0] = 0\n",
    "\n",
    "print (\"Fold: A   RMSE:{}   Best Iteration:None\".format(mean_squared_error(val_pred,train_y)**0.5) )\n",
    "print (\"Fold: A   RMSE:{}   Best Iteration:None\".format(mean_squared_error(val_pred,train_y)**0.5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: A   RMSE:1.6311168247226826   Best Iteration:None\n"
     ]
    }
   ],
   "source": [
    "print (\"Fold: A   RMSE:{}   Best Iteration:None\".format(mean_squared_error(val_pred,train_y)**0.5) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
